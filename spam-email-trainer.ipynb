{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY-D-VLbTUQv",
        "outputId": "2fdffce9-32d4-456c-a69f-35e172c857d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download successful\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "# URL of the dataset\n",
        "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "# Download the dataset\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    print(\"Download successful\")\n",
        "else:\n",
        "    print(\"Failed to download the dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the dataset\n",
        "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
        "    z.extractall(\"sms_spam_collection\")\n",
        "    print(\"Extraction successful\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3E5MF31YTn8Z",
        "outputId": "0fe15a19-d79b-4f07-8951-004aa378437d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List the extracted files\n",
        "extracted_files = os.listdir(\"sms_spam_collection\")\n",
        "print(\"Extracted files:\", extracted_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rEl9TMRTr_u",
        "outputId": "e4add1bb-fe67-4471-a89e-e439159030e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted files: ['SMSSpamCollection', 'readme']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\n",
        "    \"sms_spam_collection/SMSSpamCollection\",\n",
        "    sep=\"\\t\",\n",
        "    header=None,\n",
        "    names=[\"label\", \"message\"],\n",
        ")"
      ],
      "metadata": {
        "id": "9RIDn_TVTyYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display basic information about the dataset\n",
        "print(\"-------------------- HEAD --------------------\")\n",
        "print(df.head())\n",
        "print(\"-------------------- DESCRIBE --------------------\")\n",
        "print(df.describe())\n",
        "print(\"-------------------- INFO --------------------\")\n",
        "print(df.info())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOYV1EsCT2Hz",
        "outputId": "f8066fde-04e0-453b-d0ca-684cb6971484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------- HEAD --------------------\n",
            "  label                                            message\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
            "-------------------- DESCRIBE --------------------\n",
            "       label                 message\n",
            "count   5572                    5572\n",
            "unique     2                    5169\n",
            "top      ham  Sorry, I'll call later\n",
            "freq    4825                      30\n",
            "-------------------- INFO --------------------\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5572 entries, 0 to 5571\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   label    5572 non-null   object\n",
            " 1   message  5572 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 87.2+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicates\n",
        "print(\"Duplicate entries:\", df.duplicated().sum())\n",
        "\n",
        "# Remove duplicates if any\n",
        "df = df.drop_duplicates()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0rQfJmGT-H1",
        "outputId": "30ad73f6-7838-4f45-ecec-e11922e9f6b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate entries: 403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the necessary NLTK data files\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "print(\"=== BEFORE ANY PREPROCESSING ===\")\n",
        "print(df.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsaL336hXtjS",
        "outputId": "55f010d1-5263-4cd2-859f-4107fc5a5a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== BEFORE ANY PREPROCESSING ===\n",
            "  label                                            message\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all message text to lowercase\n",
        "df[\"message\"] = df[\"message\"].str.lower()\n",
        "print(\"\\n=== AFTER LOWERCASING ===\")\n",
        "print(df[\"message\"].head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl5X2jclXyCx",
        "outputId": "79192b77-652e-49fe-80e5-f49c1caf9a5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== AFTER LOWERCASING ===\n",
            "0    go until jurong point, crazy.. available only ...\n",
            "1                        ok lar... joking wif u oni...\n",
            "2    free entry in 2 a wkly comp to win fa cup fina...\n",
            "3    u dun say so early hor... u c already then say...\n",
            "4    nah i don't think he goes to usf, he lives aro...\n",
            "Name: message, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Remove non-essential punctuation and numbers, keep useful symbols like $ and !\n",
        "df[\"message\"] = df[\"message\"].apply(lambda x: re.sub(r\"[^a-z\\s$!]\", \"\", x))\n",
        "print(\"\\n=== AFTER REMOVING PUNCTUATION & NUMBERS (except $ and !) ===\")\n",
        "print(df[\"message\"].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOBtygWcX2up",
        "outputId": "5097e36d-bf41-4cfe-bc56-36d0bce2d154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== AFTER REMOVING PUNCTUATION & NUMBERS (except $ and !) ===\n",
            "0    go until jurong point crazy available only in ...\n",
            "1                              ok lar joking wif u oni\n",
            "2    free entry in  a wkly comp to win fa cup final...\n",
            "3          u dun say so early hor u c already then say\n",
            "4    nah i dont think he goes to usf he lives aroun...\n",
            "Name: message, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Split each message into individual tokens\n",
        "df[\"message\"] = df[\"message\"].apply(word_tokenize)\n",
        "print(\"\\n=== AFTER TOKENIZATION ===\")\n",
        "print(df[\"message\"].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1b2qehFX84-",
        "outputId": "16d9339c-701f-4143-c4a9-eb136168385e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== AFTER TOKENIZATION ===\n",
            "0    [go, until, jurong, point, crazy, available, o...\n",
            "1                       [ok, lar, joking, wif, u, oni]\n",
            "2    [free, entry, in, a, wkly, comp, to, win, fa, ...\n",
            "3    [u, dun, say, so, early, hor, u, c, already, t...\n",
            "4    [nah, i, dont, think, he, goes, to, usf, he, l...\n",
            "Name: message, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Define a set of English stop words and remove them from the tokens\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "df[\"message\"] = df[\"message\"].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "print(\"\\n=== AFTER REMOVING STOP WORDS ===\")\n",
        "print(df[\"message\"].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zmFcLG0YAG7",
        "outputId": "9adf8c38-6687-4597-9299-44c003d4915e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== AFTER REMOVING STOP WORDS ===\n",
            "0    [go, jurong, point, crazy, available, bugis, n...\n",
            "1                       [ok, lar, joking, wif, u, oni]\n",
            "2    [free, entry, wkly, comp, win, fa, cup, final,...\n",
            "3        [u, dun, say, early, hor, u, c, already, say]\n",
            "4    [nah, dont, think, goes, usf, lives, around, t...\n",
            "Name: message, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Stem each token to reduce words to their base form\n",
        "stemmer = PorterStemmer()\n",
        "df[\"message\"] = df[\"message\"].apply(lambda x: [stemmer.stem(word) for word in x])\n",
        "print(\"\\n=== AFTER STEMMING ===\")\n",
        "print(df[\"message\"].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Aqmm9v3YFL9",
        "outputId": "2828345c-fd52-483d-b623-9b475509a755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== AFTER STEMMING ===\n",
            "0    [go, jurong, point, crazi, avail, bugi, n, gre...\n",
            "1                         [ok, lar, joke, wif, u, oni]\n",
            "2    [free, entri, wkli, comp, win, fa, cup, final,...\n",
            "3        [u, dun, say, earli, hor, u, c, alreadi, say]\n",
            "4    [nah, dont, think, goe, usf, live, around, tho...\n",
            "Name: message, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rejoin tokens into a single string for feature extraction\n",
        "df[\"message\"] = df[\"message\"].apply(lambda x: \" \".join(x))\n",
        "print(\"\\n=== AFTER JOINING TOKENS BACK INTO STRINGS ===\")\n",
        "print(df[\"message\"].head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CMtg-y1YKGf",
        "outputId": "f3b488bb-ebbe-4832-d88c-6eedfd65e0ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== AFTER JOINING TOKENS BACK INTO STRINGS ===\n",
            "0    go jurong point crazi avail bugi n great world...\n",
            "1                                ok lar joke wif u oni\n",
            "2    free entri wkli comp win fa cup final tkt st m...\n",
            "3                  u dun say earli hor u c alreadi say\n",
            "4            nah dont think goe usf live around though\n",
            "Name: message, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize CountVectorizer with bigrams, min_df, and max_df to focus on relevant terms\n",
        "vectorizer = CountVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 2))\n",
        "\n",
        "# Fit and transform the message column\n",
        "X = vectorizer.fit_transform(df[\"message\"])\n",
        "\n",
        "# Labels (target variable)\n",
        "y = df[\"label\"].apply(lambda x: 1 if x == \"spam\" else 0)  # Converting labels to 1 and 0\n"
      ],
      "metadata": {
        "id": "RBQi94zGYVI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Build the pipeline by combining vectorization and classification\n",
        "pipeline = Pipeline([\n",
        "    (\"vectorizer\", vectorizer),\n",
        "    (\"classifier\", MultinomialNB())\n",
        "])"
      ],
      "metadata": {
        "id": "VHtertouYehq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    \"classifier__alpha\": [0.01, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75, 1.0]\n",
        "}\n",
        "\n",
        "# Perform the grid search with 5-fold cross-validation and the F1-score as metric\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"f1\"\n",
        ")\n",
        "\n",
        "# Fit the grid search on the full dataset\n",
        "grid_search.fit(df[\"message\"], y)\n",
        "\n",
        "# Extract the best model identified by the grid search\n",
        "best_model = grid_search.best_estimator_\n",
        "print(\"Best model parameters:\", grid_search.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AY5dR4LtYksP",
        "outputId": "c60c3a7c-748b-4734-e1a4-e45031f5a964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model parameters: {'classifier__alpha': 0.25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_messages = [\n",
        "    \"Congratulations! You've won a $1000 Walmart gift card. Go to http://bit.ly/1234 to claim now.\",\n",
        "    \"Hey, are we still meeting up for lunch today?\",\n",
        "    \"Urgent! Your account has been compromised. Verify your details here: www.fakebank.com/verify\",\n",
        "    \"Reminder: Your appointment is scheduled for tomorrow at 10am.\",\n",
        "    \"FREE entry in a weekly competition to win an iPad. Just text WIN to 80085 now!\",\n",
        "]"
      ],
      "metadata": {
        "id": "M1eK8V9mZJ9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Preprocess function that mirrors the training-time preprocessing\n",
        "def preprocess_message(message):\n",
        "    message = message.lower()\n",
        "    message = re.sub(r\"[^a-z\\s$!]\", \"\", message)\n",
        "    tokens = word_tokenize(message)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "WHNfAVWgZL_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_messages = [preprocess_message(msg) for msg in new_messages]\n"
      ],
      "metadata": {
        "id": "6UX527BjZN6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = best_model.named_steps[\"vectorizer\"].transform(processed_messages)\n"
      ],
      "metadata": {
        "id": "MYJG7jImZQLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = best_model.named_steps[\"classifier\"].predict(X_new)\n",
        "prediction_probabilities = best_model.named_steps[\"classifier\"].predict_proba(X_new)"
      ],
      "metadata": {
        "id": "Alr2x7y4ZStf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display predictions and probabilities for each evaluated message\n",
        "for i, msg in enumerate(new_messages):\n",
        "    prediction = \"Spam\" if predictions[i] == 1 else \"Not-Spam\"\n",
        "    spam_probability = prediction_probabilities[i][1]  # Probability of being spam\n",
        "    ham_probability = prediction_probabilities[i][0]   # Probability of being not spam\n",
        "\n",
        "    print(f\"Message: {msg}\")\n",
        "    print(f\"Prediction: {prediction}\")\n",
        "    print(f\"Spam Probability: {spam_probability:.2f}\")\n",
        "    print(f\"Not-Spam Probability: {ham_probability:.2f}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74enucp4ZWdR",
        "outputId": "91e5abec-af2f-4b97-e40f-98d580054c5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message: Congratulations! You've won a $1000 Walmart gift card. Go to http://bit.ly/1234 to claim now.\n",
            "Prediction: Spam\n",
            "Spam Probability: 1.00\n",
            "Not-Spam Probability: 0.00\n",
            "--------------------------------------------------\n",
            "Message: Hey, are we still meeting up for lunch today?\n",
            "Prediction: Not-Spam\n",
            "Spam Probability: 0.00\n",
            "Not-Spam Probability: 1.00\n",
            "--------------------------------------------------\n",
            "Message: Urgent! Your account has been compromised. Verify your details here: www.fakebank.com/verify\n",
            "Prediction: Spam\n",
            "Spam Probability: 0.96\n",
            "Not-Spam Probability: 0.04\n",
            "--------------------------------------------------\n",
            "Message: Reminder: Your appointment is scheduled for tomorrow at 10am.\n",
            "Prediction: Not-Spam\n",
            "Spam Probability: 0.00\n",
            "Not-Spam Probability: 1.00\n",
            "--------------------------------------------------\n",
            "Message: FREE entry in a weekly competition to win an iPad. Just text WIN to 80085 now!\n",
            "Prediction: Spam\n",
            "Spam Probability: 1.00\n",
            "Not-Spam Probability: 0.00\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the trained model to a file for future use\n",
        "model_filename = 'spam_detection_model.joblib'\n",
        "joblib.dump(best_model, model_filename)\n",
        "\n",
        "print(f\"Model saved to {model_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0ZxgAOLZdzU",
        "outputId": "3e4c0dde-857b-45bf-f715-e07cd16ac712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to spam_detection_model.joblib\n"
          ]
        }
      ]
    }
  ]
}